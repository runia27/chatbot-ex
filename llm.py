import os

from dotenv import load_dotenv
from langchain.chains import (create_history_aware_retriever,
                              create_retrieval_chain)
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import (ChatPromptTemplate, FewShotPromptTemplate,
                                    MessagesPlaceholder, PromptTemplate)
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

from config import answer_examlples


load_dotenv()

# llm í•¨ìˆ˜ ì •ì˜
def load_llm(model="gpt-4o"):
    llm = ChatOpenAI(model=model)
    return llm


# ë°ì´í„°ë² ì´ìŠ¤ í•¨ìˆ˜ ì •ì˜ 
def load_vectorstore():
    api_key = os.getenv('PINECONE_API_KEY')
    Pinecone(api_key=api_key)

    embedding = OpenAIEmbeddings(
        model="text-embedding-3-large",
    )    
   
    database = PineconeVectorStore.from_existing_index(
        embedding=embedding,
        index_name="law1and2-quiz4",
    )
    return database

# ì„¸ì…˜ ì•„ì´ë”” ìƒì„±
store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


# íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ë¦¬íŠ¸ë¦¬ë²„
def build_history_aware_retriever(llm, retriever):

    question_prompt = ('''
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì°¸ì¡°í•œë‹¤ë©´, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëˆ„êµ¬ë‚˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.
- ì§ˆë¬¸ì´ ì´ë¯¸ ë…ë¦½ì ì¸ ë¬¸ì¥ì´ë¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
- ë‹µë³€ì€ í•˜ì§€ ì•Šê³ , ì˜¤ì§ ì§ˆë¬¸ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
- í•­ìƒ ì§ˆë¬¸ í˜•íƒœë¡œ ëë‚˜ì•¼ í•˜ë©°, ë¬¸ì¥ì€ ì •í™•í•˜ê³  ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.

''')

    contextualize_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    history_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_prompt
    )
    return history_retriever


## few shot
def build_few_shot_examples() -> str:

    example_prompt = PromptTemplate.from_template("Question: {input}\n{answer}")

    few_shot_prompt = FewShotPromptTemplate(
        examples=answer_examlples,
        example_prompt=example_prompt,
        prefix= "ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš” : ",
        suffix="Question: {input}",
        input_variables=["input"],
    )
   
    formated_few_shot_prompt = few_shot_prompt.format(input="{input}")
    return formated_few_shot_prompt
    

# 
def build_qa_prompt():
    prompt = ('''
[Identity]
- ë‹¹ì‹ ì€ ì „ì„¸ì‚¬ê¸° í”¼í•´ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
- "context"ë¥¼ ì°¸ê³ í•´ ì§ˆë¬¸ì— ëŒ€í•´ ì¼ëª©ìš”ì—°í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. 
- í•­ëª©ë³„ë¡œ í‘œì‹œí•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
- ë‹µë³€ ë§ˆì§€ë§‰ ë¶€ë¶„ì—ëŠ” ê´€ë ¨ ë²•ì¡°í•­ì„ ì†Œê´„í˜¸ ì•ˆì— ë„£ì–´ ê°™ì´ ëª…ì‹œí•´ì£¼ì„¸ìš”. 
- ì–´ë–¤ ê²ƒì— ëŒ€í•´ ì•Œë ¤ë‹¬ë¼ëŠ” ì§ˆë¬¸ì€ ì–´ë–¤ ê²ƒì— ëŒ€í•´ ì„¤ëª…í•´ë‹¬ë¼ëŠ” ë§ê³¼ ê°™ìŠµë‹ˆë‹¤. 
- ì‚¬ìš©ìë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
- ê´€ë ¨ë˜ì§€ ì•Šì€ ì§ˆë¬¸ì—ëŠ” ì „ì„¸ì‚¬ê¸°ì™€ ê´€ë ¨ëœ ë¹„ìŠ·í•œ ì§ˆë¬¸ì„ ë¬¼ì–´ë³¸ê²Œ ë§ëŠ”ì§€ ë˜ë¬»ìŠµë‹ˆë‹¤.

context: {context}       
question: {input}                                   
answer:
    ''')

    formated_few_shot_prompt = build_few_shot_examples()

    qa_prompt = ChatPromptTemplate.from_messages([
    ("system", prompt),
    ("assistant", formated_few_shot_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
    ])
    
    return qa_prompt


# RetrievalQA í•¨ìˆ˜ ì •ì˜
def get_all_chain():

    database = load_vectorstore()
    retriever = database.as_retriever(search_kwargs={'k': 2})
    llm = load_llm()

    history_retriever = build_history_aware_retriever(llm, retriever)
    qa_prompt = build_qa_prompt()
    
    answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_retriever, answer_chain)

    all_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer"
    ).pick("answer")

    return all_chain


## AI ë©”ì„¸ì§€ í•¨ìˆ˜ ì •ì˜ 
def stream_get_aimessage(user_message, session_id='default'):        
    all_chain = get_all_chain()

    aimessage = all_chain.stream(
        {"input": user_message},
        config={"configurable":{"session_id": session_id}},
    )

    print(f"ëŒ€í™”ì´ë ¥: {get_session_history(session_id).messages} \nğŸ¾\n")
    print('=' * 50)
    print(f"{session_id}")

    return aimessage

